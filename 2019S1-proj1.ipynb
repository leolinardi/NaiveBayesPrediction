{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Leonardo Linardi (855915)\n",
    "###### Python version: 3\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing external modules\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    \"\"\"\n",
    "        // MAIN FUNCTION\n",
    "        Opens a data file in csv specified by its 'filename' and transform it into a 'dataset' (list of list of instances).\n",
    "    \"\"\"\n",
    "    lines = csv.reader(open(filename, \"r\"))\n",
    "    dataset = list(lines)\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = [x for x in dataset[i]]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(dataset):\n",
    "    \"\"\"\n",
    "        // HELPER FUNCTION\n",
    "        Given a 'dataset' (list of instances), returns the class label of all instances.     \n",
    "    \"\"\"\n",
    "    dataset_array = np.array(dataset)\n",
    "    class_labels = dataset_array[:,-1]\n",
    "    return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prior(dataset):\n",
    "    \"\"\"\n",
    "        // HELPER FUNCTION\n",
    "        Given a 'dataset' (list of instances), calculates the prior probabilities representing the \n",
    "        class distribution P(c). Returns a model (a dictionary, keys are class labels, values are counts or probabilities).\n",
    "    \"\"\"\n",
    "    prior_prob = defaultdict(int)\n",
    "    class_labels = get_class_labels(dataset)\n",
    "    \n",
    "    # calculate the frequency of each class label\n",
    "    for lab in class_labels:\n",
    "        prior_prob[lab] += 1\n",
    "    \n",
    "    # calculate the prior probability of each class label\n",
    "    total_inst = len(dataset)\n",
    "    for label in prior_prob:\n",
    "        prior_prob[label] = prior_prob[label]/total_inst\n",
    "    \n",
    "    return prior_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_posterior(dataset):\n",
    "    \"\"\"\n",
    "        // HELPER FUNCTION\n",
    "        Given a 'dataset' (list of instances), calculates the posterior probabilities representing the \n",
    "        conditional probabilities P(a|c). Returns a model (a list (one per attribute) of lists (one per class) of\n",
    "        dictionaries (keys are attribute values, values are counts or probabilities)).\n",
    "    \"\"\"\n",
    "    posterior_count = []\n",
    "    \n",
    "    # stores the frequency of attributes per class label, for each attribute of the dataset\n",
    "    for att in range(len(dataset[0])-1):\n",
    "        class_dict = {}\n",
    "\n",
    "        for inst in range(len(dataset)):\n",
    "            att_value = dataset[inst][att]\n",
    "            inst_label = dataset[inst][-1]\n",
    "            \n",
    "            # a dictionary hasn't exist yet for that label\n",
    "            if inst_label not in class_dict:\n",
    "                class_dict[inst_label] = defaultdict(int)\n",
    "                class_dict[inst_label][att_value] += 1\n",
    "            else:\n",
    "                att_dict = class_dict[inst_label]\n",
    "                att_dict[att_value] += 1\n",
    "                class_dict[inst_label] = att_dict\n",
    "        \n",
    "        posterior_count.append(class_dict)\n",
    "    \n",
    "    return posterior_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset):\n",
    "    \"\"\"\n",
    "        // MAIN FUNCTION\n",
    "        Given a 'dataset' (list of instances), returns a 2-tuple of trained model, where\n",
    "        a dictionary, keys are class labels, values are counts or probabilities\n",
    "        prior_model     : a dictionary (keys are class labels, values are counts or probabilities) of probabilities \n",
    "                          (i.e. normalised counts) representing the class distribution P(c)\n",
    "        posterior_model : a list (one per attribute) of lists (one per class) of dictionaries \n",
    "                          (keys are attribute values, values are counts or probabilities) representing the conditional \n",
    "                          probabilities P(a|c)\n",
    "    \"\"\"\n",
    "    prior_model = train_prior(dataset)\n",
    "    posterior_model = train_posterior(dataset)    \n",
    "    return (prior_model, posterior_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_att_per_class(freqdict):\n",
    "    \"\"\"\n",
    "        // HELPER FUNCTION\n",
    "        Finds the total frequency of all attributes of a given class. \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for label, ct in freqdict.items():\n",
    "        total += ct\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_score(scores_dict):\n",
    "    \"\"\"\n",
    "        // HELPER FUNCTION\n",
    "        Finds the maximum score of class labels that a test instance could have,\n",
    "        when predicting the label of that instance.\n",
    "    \"\"\"\n",
    "    max_score = max(scores_dict.values())\n",
    "    for label, ct in scores_dict.items():\n",
    "        if ct == max_score:\n",
    "            return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.0001\n",
    "CONSTANT_K = 0.1\n",
    "\n",
    "def smoothing(label_counts_ddict, inst_att, method):\n",
    "    \"\"\"\n",
    "        // HELPER FUNCTION\n",
    "        Does the smoothing process upon calculating the frequency of attributes of a given class.\n",
    "        The smoothing regime is done based on the method specified, which are:\n",
    "        1: Epsilon Smoothing\n",
    "        2: Laplace Smoothing (add-one smoothing)\n",
    "        3: Add-k Smoothing\n",
    "    \"\"\"\n",
    "    freq = label_counts_ddict[inst_att]\n",
    "    total_freq = total_att_per_class(label_counts_ddict)\n",
    "    num_of_att = len(label_counts_ddict)\n",
    "    if method == 1:\n",
    "        if freq == 0:\n",
    "            freq = EPSILON\n",
    "        return freq/total_freq\n",
    "    elif method == 2:\n",
    "        freq += 1\n",
    "        total_freq += num_of_att\n",
    "        return freq/total_freq\n",
    "    elif method == 3:\n",
    "        freq += CONSTANT_K\n",
    "        total_freq += (num_of_att*CONSTANT_K)\n",
    "        return freq/total_freq\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_VAL = '?'\n",
    "\n",
    "def predict(model, testset):\n",
    "    \"\"\"\n",
    "        // MAIN FUNCTION\n",
    "        Predicts the class labels for each instance in a 'testset' (a list of instances), based on a \n",
    "        trained 'model' (2-tuple of prior and posterior model). Returns a list of labels that are predicted.\n",
    "    \"\"\"\n",
    "    prior_model = model[0]\n",
    "    posterior_model = model[1]\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # iterate through each test set, and determines which class label has the highest score\n",
    "    for inst in testset:\n",
    "        label_scores = {}\n",
    "        for label in prior_model:\n",
    "            curr_score = 0\n",
    "            for att in range(len(inst)-1):\n",
    "                att_value = inst[att]\n",
    "                \n",
    "                # ignores missing attribute values for test instances\n",
    "                if att_value != MISSING_VAL:\n",
    "                    label_counts_dict = posterior_model[att][label]\n",
    "                    freq_count = smoothing(label_counts_dict, att_value, 3)\n",
    "                    curr_score *= freq_count\n",
    "                    \n",
    "            # stores the score of the current label\n",
    "            curr_score *= prior_model[label]\n",
    "            label_scores[label] = curr_score\n",
    "            \n",
    "        predicted_labels.append(maximum_score(label_scores))\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testset):\n",
    "    \"\"\"\n",
    "        // MAIN FUNCTION\n",
    "        Evaluates a set of class label predictions, in a supervised context. Where the 'testset' is \n",
    "        a list of instances, based on a trained 'model' (2-tuple of prior and posterior model). \n",
    "        Uses the Accuracy method to evaluate the predictions.\n",
    "    \"\"\"\n",
    "    actual_labels = get_class_labels(testset)\n",
    "    predicted_labels = predict(model, testset)\n",
    "    \n",
    "    # evaluates the performance of the classifier\n",
    "    correct = 0\n",
    "    for i in range(len(actual_labels)):\n",
    "        if actual_labels[i] == predicted_labels[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct/len(actual_labels)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(dataset):\n",
    "    \"\"\"\n",
    "        // MAIN FUNCTION\n",
    "        Given a 'dataset' (list of instances), this function calculates and returns a list of \n",
    "        Information Gain of each attribute.\n",
    "    \"\"\"\n",
    "    prior_model = train(dataset)[0]\n",
    "    all_info_gain = []\n",
    "    \n",
    "    for att in range(len(dataset[0])-1):\n",
    "        \n",
    "        # finds the class distribution of an attribute\n",
    "        class_dist = {}\n",
    "        for inst in range(len(dataset)):\n",
    "            att_value = dataset[inst][att]\n",
    "            inst_label = dataset[inst][-1]\n",
    "            \n",
    "            # a dictionary hasn't exist yet for that label\n",
    "            if att_value not in class_dist:\n",
    "                class_dist[att_value] = defaultdict(int)\n",
    "                class_dist[att_value][inst_label] += 1 \n",
    "            else:\n",
    "                att_dict = class_dist[att_value]\n",
    "                att_dict[inst_label] += 1\n",
    "                class_dist[att_value] = att_dict\n",
    "        \n",
    "        info_gain = 0\n",
    "        \n",
    "        # calculate the entropy for all of the instances\n",
    "        total_freq = total_att_per_class(prior_model)\n",
    "        entropy = 0\n",
    "        for cls, freq in prior_model.items():\n",
    "            prob = freq/total_freq\n",
    "            entropy += (prob)*(math.log(prob, 2))\n",
    "        info_gain += (-entropy)\n",
    "        \n",
    "        # calculate the entropy of the class distribution\n",
    "        mean_info = 0\n",
    "        for att, ddict in class_dist.items():\n",
    "            entropy = 0\n",
    "            total_freq = total_att_per_class(ddict)\n",
    "            \n",
    "            for cls, freq in ddict.items():\n",
    "                prob = freq/total_freq\n",
    "                entropy += (prob)*(math.log(prob, 2))\n",
    "            mean_info += (total_freq/len(dataset))*(-entropy)\n",
    "\n",
    "        info_gain -= mean_info\n",
    "        all_info_gain.append(info_gain)\n",
    "        \n",
    "    return all_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Questions Responded: 1 and 5\n",
    "The answers are on a seperate document under the name '855915 - Project 1 Written Answers.pdf' in this zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
